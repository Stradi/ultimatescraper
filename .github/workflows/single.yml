name: Scrape Single Comic
run-name: Scrape ${{ inputs.url }} from ${{ inputs.provider }}
on:
  workflow_dispatch:
    inputs:
      url:
        description: "URL of the comic"
        required: true
        type: string
        default: "https://www.viewcomics.me/comic/COMIC_SLUG"
      provider:
        description: "Provider of choice"
        required: false
        type: choice
        default: "ViewComics"
        options:
          - ViewComics
      log_level:
        description: "Log level of Scrapy"
        required: false
        type: choice
        default: "INFO"
        options:
          - "DEBUG"
          - "INFO"
          - "WARNING"
jobs:
  scrape:
    name: Scrapes the comic from ViewComics and adds to the database
    runs-on: ubuntu-latest
    env:
      DB_HOST: ${{ secrets.DB_HOST }}
      DB_USER: ${{ secrets.DB_USER }}
      DB_PASS: ${{ secrets.DB_PASS }}
      DB_NAME: ${{ secrets.DB_NAME }}
      DB_SSL_CERT: cacert.pem
      REVALIDATION_URL: ${{ secrets.REVALIDATION_URL }}
      REVALIDATION_SECRET: ${{ secrets.REVALIDATION_SECRET }}
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10.8"
          cache: "pip"
      - name: Install dependencies via Pip
        run: pip install -r requirements.txt
      - name: Run scraper
        run: scrapy crawl ${{ inputs.provider }} -a comic=${{ inputs.url }} -s LOG_LEVEL=${{ inputs.log_level }}
